{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsbguCuioQ9a"
   },
   "source": [
    "# Stereo Matching with Pytorch\n",
    "\n",
    "## **Part 2: Learning based stereo matching**\n",
    "      \n",
    "(1) Train            \n",
    "(2) Modifying different parameters for better performance           \n",
    "(3) Evaluate stereo matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTDDcmhPoi2v"
   },
   "source": [
    "Rather than of using SSD/SAD to compute a matching cost for the disparity map, in this part we will train a network to directly learn that from the data instead. We will implement what has been proposed in the paper [[Zbontar & LeCun, 2015]](https://arxiv.org/abs/1409.4326) and see how it performs compare to classical cost matching approaches.\n",
    "\n",
    "**Note:**\n",
    "This notebook is intendend to be run on [Google Colab](https://colab.research.google.com). So first upload the notebook to Google Colab, and then select \"GPU\" in the menu \"Runtime\" -> \"Change runtime time\" -> \"Hardward accelerator\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qEqq1Gvts-c"
   },
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDNH_AnAt0oo"
   },
   "source": [
    "Here we will download neccessary data and set up the environment. You can skip this if the data files is still in your colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1T15O22mwGnE",
    "nbdime-conflicts": {
     "local_diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "3be4a73c-bcce-42bc-8beb-7e06134e2eba"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "outputId",
       "op": "patch"
      }
     ],
     "remote_diff": [
      {
       "key": "outputId",
       "op": "remove"
      }
     ]
    }
   },
   "outputs": [],
   "source": [
    "# # download and extract the data -- training data\n",
    "!rm Middlebury_data.zip\n",
    "!wget \"https://onedrive.live.com/download?cid=21FDB23F3688A415&resid=21FDB23F3688A415%2140534&authkey=AJX_HsPo6T6wJV8\" -O Middlebury_data.zip && unzip Middlebury_data.zip\n",
    "\n",
    "# # download and extract the data for the appropriate window size -- pre-trained network\n",
    "!rm mc_cnn_network_pretrain_*\n",
    "\n",
    "!wget \"https://onedrive.live.com/download?cid=D9AE7CB176DB9D55&resid=D9AE7CB176DB9D55%21121&authkey=AIoYwMTUWB7ekgE\" -O mc_cnn_network_pretrain_ws11.pth\n",
    "!wget \"https://onedrive.live.com/download?cid=D9AE7CB176DB9D55&resid=D9AE7CB176DB9D55%21119&authkey=AGjnpMZXwihXW_8\" -O mc_cnn_network_pretrain_ws5.pth \n",
    "!wget \"https://onedrive.live.com/download?cid=D9AE7CB176DB9D55&resid=D9AE7CB176DB9D55%21120&authkey=AE_5NGf-Qk13YJ0\" -O mc_cnn_network_pretrain_ws9.pth \n",
    "!wget \"https://onedrive.live.com/downloaD?cid=D9AE7CB176DB9D55&resid=D9AE7CB176DB9D55%21122&authkey=AO9QfmLdbpp2cyA\" -O mc_cnn_network_pretrain_ws15.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1o7JCNnIy9Q"
   },
   "source": [
    "## Compute Requirements\n",
    "\n",
    "You can try out Google Colab to run this notebook. These are the steps we follow:\n",
    "1. Upload this notebook to google colab\n",
    "2. Zip the folders ```proj4_code```, `proj4_unit_tests` and `semiglobalmatching` in the project directory and upload it to the colab runtime\n",
    "3. Unzip the uploaded zip using ```!unzip -qq <uploaded_file>.zip -d ./```\n",
    "\n",
    "Remember to download all the files saved and changes to code you made on the colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2eoM7o-8V4Z"
   },
   "outputs": [],
   "source": [
    "!unzip -qq proj4.zip -d ./\n",
    "\n",
    "# delete folder\n",
    "# !rm -r proj4_code\n",
    "# !rm proj4_code.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXj3N8uhtcEZ",
    "nbdime-conflicts": {
     "local_diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "9b5234e0-60fe-483d-fae5-1ecc5797d6e8"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "outputId",
       "op": "patch"
      }
     ],
     "remote_diff": [
      {
       "key": "outputId",
       "op": "remove"
      }
     ]
    }
   },
   "outputs": [],
   "source": [
    "# import torch and set tensor type\n",
    "import torch\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "use_cuda = True and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "tensor_type = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "torch.set_default_tensor_type(tensor_type)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(333) # do not change this, this is to ensure your result is reproducible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbWgBKBdxRbD"
   },
   "source": [
    "Run this cell to be able to reload any changes you make to your code on the fly.\n",
    "\n",
    "You can access your source code in the left-hand side menu under the folder icon. You can use the built-in editing tools to fix any bugs right here in colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uwgk8oUBqHug"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXjPIipCzGag"
   },
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdK2HbLbzKTz"
   },
   "source": [
    "We have already stored all the training data in the `.npy` format, now we just need to simply load them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PebRPqmZ73tX"
   },
   "outputs": [],
   "source": [
    "# load up all data for training and testing\n",
    "from proj4_code.dataset import loadbin, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "data_dir = 'Middlebury_data'\n",
    "\n",
    "# the img to show\n",
    "ind_img = \"Bicycle1\"\n",
    "\n",
    "# pick another 4 images for testing\n",
    "test_scenes = [ind_img]\n",
    "test_scenes += [\"Adirondack\", \"Flowers\", \"Playroom\", \"Recycle\"]\n",
    "\n",
    "# pick the left for training\n",
    "train_scenes = [\n",
    "    \"Playtable\",\n",
    "    \"Shelves\",\n",
    "    \"Vintage\",\n",
    "    \"Backpack\",\n",
    "    \"Cable\",\n",
    "    \"Classroom1\",\n",
    "    \"Couch\",\n",
    "    \"Mask\",\n",
    "    \"Shopvac\",\n",
    "    \"Sticks\",\n",
    "    \"Storage\",\n",
    "    \"Sword1\",\n",
    "    \"Sword2\",\n",
    "    \"Umbrella\",\n",
    "]\n",
    "\n",
    "val_scenes = [\"Jadeplant\", \"Motorcycle\", \"Piano\", \"Pipes\"]\n",
    "\n",
    "TrainData = DataLoader(data_dir, train_scenes, ws=11)\n",
    "TestData = DataLoader(data_dir, test_scenes, ws=11, num_batches_per_epoch=20)\n",
    "ValData = DataLoader(data_dir, val_scenes, ws=11, num_batches_per_epoch=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qturMloRxpeb"
   },
   "source": [
    "### Visualizing the Data\n",
    "Let's visualize an example from the test set. We'll be visualising the disparity maps we compute for this image later on.*italicized text*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f05xFB1hNavv",
    "nbdime-conflicts": {
     "local_diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "582a3af0-d0b6-4ae1-bcca-b798898a455f"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "outputId",
       "op": "patch"
      }
     ],
     "remote_diff": [
      {
       "key": "outputId",
       "op": "remove"
      }
     ]
    }
   },
   "outputs": [],
   "source": [
    "# visualize the test data\n",
    "print('Left Image')\n",
    "plt.imshow(TestData.imgs_left[ind_img][0].cpu().numpy(),cmap=\"gray\")\n",
    "plt.show()\n",
    "print('Right Image')\n",
    "plt.imshow(TestData.imgs_right[ind_img][0].cpu().numpy(),cmap=\"gray\")\n",
    "plt.show()\n",
    "print('Ground Truth disparity')\n",
    "plt.imshow(TestData.disps[ind_img], cmap='jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bwq02eqpNbTq"
   },
   "source": [
    "## Train MCNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GdYb7H8xLDM"
   },
   "source": [
    "Train the network that learn how to classify 2 patches as positive vs negative match by experimenting with the learning parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxPmtAdlxmc-"
   },
   "source": [
    "The training process works as follows:\n",
    "\n",
    "1. In every iteration, we find non-zero disparity values and sample patches around a random selection of those pixels with `gen_patch`.\n",
    "2. We construct positive and negative examples for those patches by correctly shifting the right image in the positive sample, and incorrectly shifting it with some random jitter within some bound.\n",
    "3. We collate these positive and negative examples into a batch, and pass that to the model.\n",
    "4. We evaluate the loss, and backpropagate the changes to adjust the model's weights.\n",
    "5. An epoch is an iteration over a number of batches of these samples. Each epoch is large enough to cover a representative sample of the dataset, but does not cover the entire dataset due to time-constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JRBI6guyCEAe"
   },
   "outputs": [],
   "source": [
    "from proj4_code.utils import save_model, load_model\n",
    "from proj4_code.part2a_network import MCNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0NIVdSF_Hv34"
   },
   "outputs": [],
   "source": [
    "from proj4_code.train import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fzVsB_GyM6C"
   },
   "source": [
    "## Changing hyperparameters\n",
    "\n",
    "The block below allows you to set up your model before the training commences.\n",
    "\n",
    "You can use this to set the values for your hyperparameters.\n",
    "\n",
    "Run the notebook once without changing the settings, and then try some different ones out.\n",
    "\n",
    "### Vary window size\n",
    "\n",
    "Experiment with different window size, namely 5x5, 9x9, 11x11 and 15x15 and compare the performance. You can set this in the code block below for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECQt-mL8fmV-",
    "nbdime-conflicts": {
     "local_diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "03d67a1a-20ea-44f0-94a9-d0682c44ca2a"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "outputId",
       "op": "patch"
      }
     ],
     "remote_diff": [
      {
       "key": "outputId",
       "op": "remove"
      }
     ]
    }
   },
   "outputs": [],
   "source": [
    "############# EXPERIMENT WITH THESE ########\n",
    "######################################################################\n",
    "# You can also train from scratch by setting load_path to None\n",
    "#######################################################################\n",
    "\n",
    "######STUDENT CODE HERE##########\n",
    "# you can change this to load up pretrain network for other window size\n",
    "# pick your favorite window size (from the pretrained sizes available)\n",
    "ws = 11 \n",
    "batch_size = 128\n",
    "#################################\n",
    "\n",
    "net_train = MCNET(ws=ws,load_path = f\"mc_cnn_network_pretrain_ws{ws}.pth\",strict=True, batch_size=batch_size)\n",
    "# net_train = MCNET(ws=11,load_path = None,strict=True, batch_size=batch_size) \n",
    "\n",
    "max_epoch = 50\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(net_train.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.SGD(net_train.net.parameters(), lr=learning_rate, momentum=0.9) # Uncomment this to use the SGD optimizer\n",
    "\n",
    "############################################\n",
    "# generate train dataloader and test dataloader\n",
    "TrainData = DataLoader(data_dir, train_scenes, ws=ws, batch_size=batch_size, num_batches_per_epoch=512)\n",
    "TestData = DataLoader(data_dir, test_scenes, ws=ws, batch_size=batch_size, num_batches_per_epoch=8)\n",
    "ValData = DataLoader(data_dir, val_scenes, ws=ws, batch_size=batch_size,  num_batches_per_epoch=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqAB4jI0zNY7"
   },
   "source": [
    "## Start Training!\n",
    "\n",
    "Now that you have set up your model, you can start training it. It might take you 20-30 minutes to complete the training.\n",
    "\n",
    "You can use the `resume_training` option if you interrupt your training, and Colab hasn't reset your runtime and deleted the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5eMOnlucEoMv",
    "nbdime-conflicts": {
     "local_diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "61bfe7fd-1c3b-4ff0-e804-0638bdfe7a0d"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "outputId",
       "op": "patch"
      }
     ],
     "remote_diff": [
      {
       "key": "outputId",
       "op": "remove"
      }
     ]
    }
   },
   "outputs": [],
   "source": [
    "# train\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "st = time.time()\n",
    "\n",
    "viz_frequency = 5\n",
    "save_frequency = 10 #for saving model\n",
    "\n",
    "#########################################################################################\n",
    "# In case training is interrupted, \n",
    "# change the `resume_training` flag to True.\n",
    "#\n",
    "# Make sure to have the model weights file (e.g. mc_cnn_network_ws11.pth)\n",
    "# and the checkpoint file (e.g. train_checkpoint_mc_cnn_ws11.pth) in the same folder\n",
    "trainer = Trainer(model=net_train,\n",
    "                       dataloader=TrainData,\n",
    "                       batch_size=batch_size, \n",
    "                       ws=ws, \n",
    "                       max_epoch=max_epoch,\n",
    "                       optimizer=optimizer,\n",
    "                       viz_frequency=viz_frequency, \n",
    "                       save_frequency=save_frequency,\n",
    "                       fname = f'mc_cnn_network_ws{ws}.pth',\n",
    "                       resume_training=False,\n",
    "                       checkpoint_path=f\"train_checkpoint_mc_cnn_ws{ws}.pth\")\n",
    "\n",
    "train_loss_history, train_acc_history, val_loss_history, val_acc_history = trainer.train(ValData)\n",
    "print('Time elasped: ', time.time() - st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xWbgWVbxJ2Z"
   },
   "source": [
    "The network will be saved in `mc_cnn_network_vanilla.pth` or whatever name you set, this is the final saved model you'll need to run other part of the project. You can download a copy of this from colab so that you don't lost progress if anything goes wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REkhaqf1D8zi"
   },
   "source": [
    "Experiment with different network parameters(window size, optimizer type, etc.) and answer the reflection questions in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hz_8YKjgAy6"
   },
   "source": [
    "\n",
    "### Visualize training curves\n",
    "\n",
    "Let's take a look at the training and validation losses and accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UYRN7CoCmzTl"
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(5*4)\n",
    "ax1.plot(train_loss_history, label=\"train loss\")\n",
    "ax1.plot(val_loss_history, label=\"val loss\")\n",
    "ax1.set_title(\"Loss Plot\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(train_acc_history, label=\"train accuracy\")\n",
    "ax2.plot(val_acc_history, label=\"val accuracy\")\n",
    "ax2.legend()\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_title(\"Accuracy Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gg-fs8MZpZcy"
   },
   "source": [
    "## Evaluate stereo matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8a7_tKRUpx1c"
   },
   "source": [
    "Now that we have a trained model ready, let's visualize some results!\n",
    "\n",
    "Our eventual goal is to generate the disparity maps using our matching-cost network.\n",
    "\n",
    "The next few cells will guide you through this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2AcNsIRqABs6"
   },
   "outputs": [],
   "source": [
    "###########Change to your best model here################\n",
    "\n",
    "net_test = MCNET(batch_size = 1, load_path = f'mc_cnn_network_ws{ws}.pth')\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZBTTD5rr4ga"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from proj4_code.part1b_similarity_measures import sad_similarity_measure,ssd_similarity_measure\n",
    "  from proj4_code.part1c_disparity_map import calculate_disparity_map\n",
    "  from proj4_code.part2c_disparity import calculate_mccnn_disparity_map, mc_cnn_similarity\n",
    "except ModuleNotFoundError:\n",
    "  print('\\033[91m Error: please upload disparity_map.py and similarity_measures.py from part 1\\033[0m ')\n",
    "from proj4_code.dataset import loadbin, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "    \n",
    "scale = 5\n",
    "im_left = TestData.imgs_left[ind_img][0][::scale,::scale]\n",
    "im_right = TestData.imgs_right[ind_img][0][::scale,::scale]\n",
    "im_dispnoc = TestData.disps[ind_img][::scale,::scale]\n",
    "im_dispnoc_full = TestData.disps[ind_img]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8IgonoG0Vf3"
   },
   "source": [
    "### Visualizing the ground-truth\n",
    "\n",
    "Once again, let's take a look at the ground-truth images, and the best-case disparity map that can be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTkzNpmHsjHb",
    "nbdime-conflicts": {
     "local_diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "17f90866-e291-4650-b329-fe9635669b9a"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "outputId",
       "op": "patch"
      }
     ],
     "remote_diff": [
      {
       "key": "outputId",
       "op": "remove"
      }
     ]
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3)\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(5*3)\n",
    "\n",
    "ax1.imshow(im_left.cpu().numpy(), cmap=\"gray\")\n",
    "ax1.title.set_text('Left image')\n",
    "\n",
    "ax2.imshow(im_right.cpu().numpy(), cmap=\"gray\")\n",
    "ax2.title.set_text('Right image')\n",
    "\n",
    "ax3.imshow(im_dispnoc/scale, cmap=\"jet\")\n",
    "ax3.title.set_text('Ground Truth disparity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esqtppwM0kOp"
   },
   "source": [
    "## Extract network components\n",
    "\n",
    "The feature extractor (convolutional layers) and the classifier (the fully-connected layers) are both independently useful parts of the network.\n",
    "\n",
    "Let's extract them from our network so we can use them independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7EeSuTDDKu2_"
   },
   "outputs": [],
   "source": [
    "# Get the feature extractor and fully-connected layers from trained model \n",
    "feature_extractor = net_test.conv\n",
    "fc_layers = net_test.classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sr4TMjW107OE"
   },
   "source": [
    "### Extract Conv Features\n",
    "\n",
    "We use the feature extractor to extract convolutional features from the left and right images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ovf3ThGKrpL"
   },
   "outputs": [],
   "source": [
    "# Extract CNN features from left and right images\n",
    "with torch.no_grad(): # Don't need to compute gradients here\n",
    "  cnn_left_img = feature_extractor(im_left.cuda().unsqueeze(0).unsqueeze(0))\n",
    "  cnn_right_img = feature_extractor(im_right.cuda().unsqueeze(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmNYyFVG1YqB"
   },
   "source": [
    "# Visualizing the error profile of our trained network\n",
    "\n",
    "Let's extract a patch, and a window to search from the left and right images.\n",
    "\n",
    "We'll then use these to analyse the error profiles of our trained network, and compare them to the error profile from the SAD and SSD metrics.\n",
    "\n",
    "### 1. Extract and visualize the patches\n",
    "\n",
    "The cell below will extract an \"interesting\" patch for us to analyze the error profile for, and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTGNZIoWs_4f",
    "nbdime-conflicts": {
     "local_diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "e38fb59e-ae04-42bc-8bc6-7d711ce165c4"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "outputId",
       "op": "patch"
      }
     ],
     "remote_diff": [
      {
       "key": "outputId",
       "op": "remove"
      }
     ]
    }
   },
   "outputs": [],
   "source": [
    "# extract a patch of interest from the left image, and it's CNN features\n",
    "patch_left_img = im_left[80:91, 85:96]\n",
    "cnn_patch_left_img = cnn_left_img[...,80:91, 85:96]\n",
    "\n",
    "# get the search area in the right image\n",
    "search_area_right_img = im_right[80:91, 60:102]\n",
    "cnn_search_area_right_img = cnn_right_img[...,80:91, 60:102]\n",
    "\n",
    "\n",
    "plt.imshow(patch_left_img.cpu().numpy(), cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(search_area_right_img.cpu().numpy(),cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iupGIOmz1uFp"
   },
   "source": [
    "### Visualizing the error profiles\n",
    "Now that we have the patches, we can use our similarity measures, one from SAD, one from SSD, and the final one from the MC-CNN using the convolutional features of the patches we extracted.\n",
    "\n",
    "**Note:** Your error profile might come out non-convex, in which case your network has learned poorly. Due to the stochastic nature of neural networks, and this specific algorithm, you aren't guaranteed to get a great profile. Please try training the network again to make sure you have a convex error profile with the minimum error somewhere between index 10 and 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Ijk3zPLqvCH",
    "nbdime-conflicts": {
     "local_diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "7d3ed8dd-68e7-44ac-ce57-3ed5697aca6b"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "outputId",
       "op": "patch"
      }
     ],
     "remote_diff": [
      {
       "key": "outputId",
       "op": "remove"
      }
     ]
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,3))\n",
    "\n",
    "similarity_vals = np.array([sad_similarity_measure(patch_left_img, search_area_right_img[:,h_idx:(h_idx+11)]) for h_idx in range(search_area_right_img.shape[1]-10)])\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(similarity_vals)\n",
    "plt.title('SSD')\n",
    "\n",
    "similarity_vals = np.array([ssd_similarity_measure(patch_left_img, search_area_right_img[:,h_idx:(h_idx+11)]) for h_idx in range(search_area_right_img.shape[1]-10)])\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(similarity_vals)\n",
    "plt.title('SAD')\n",
    "\n",
    "similarity_vals = mc_cnn_similarity(fc_layers,\n",
    "                                    cnn_patch_left_img,\n",
    "                                    torch.cat([\n",
    "                                      cnn_search_area_right_img[...,h_idx:(h_idx+11)]\n",
    "                                      for h_idx in range(search_area_right_img.shape[1]-10)],\n",
    "                                      dim=0)).cpu().numpy()\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(similarity_vals)\n",
    "plt.title('MC-CNN')\n",
    "fig.suptitle('Matching cost comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIwX6RM63COA"
   },
   "source": [
    "# Calculate the Disparity map\n",
    "\n",
    "We calculate the disparity map using the functions you implemented in part1, as well as the `calculate_mc_cnn_disparity_map` function from `part2c_disparity`.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "**PLEASE** ensure that your method is efficient, as this is a long process, and will take a few minutes at the very least. A careless (and inefficient) implementation can take _hours_ to run which will be frustrating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BDJhxZxRU8U-",
    "nbdime-conflicts": {
     "local_diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "e88395ab-adbc-4b5e-ac90-91ba700c16c7"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "outputId",
       "op": "patch"
      }
     ],
     "remote_diff": [
      {
       "key": "outputId",
       "op": "remove"
      }
     ]
    }
   },
   "outputs": [],
   "source": [
    "#calculate the disparity map with block size of 11, this will take a while\n",
    "st = time.time()\n",
    "disp_map = calculate_disparity_map((im_left.unsqueeze(0).transpose(2,0).transpose(0,1)), #swap the channel\n",
    "                                   (im_right.unsqueeze(0).transpose(2,0).transpose(0,1)), \n",
    "                                   block_size=11, \n",
    "                                   sim_measure_function = sad_similarity_measure,\n",
    "                                   max_search_bound = 30)\n",
    "print('Time Elasped for sad_similarity_measure:', time.time() - st)\n",
    "\n",
    "st = time.time()\n",
    "disp_map_cnn = calculate_mccnn_disparity_map(net_test,\n",
    "                                             im_left.unsqueeze(0),\n",
    "                                             im_right.unsqueeze(0),\n",
    "                                             block_size = 11,\n",
    "                                             sim_measure_function = mc_cnn_similarity,\n",
    "                                             max_search_bound = 30)\n",
    "print('Time Elasped for cnn_similarity_measure:', time.time() - st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHy80TSy_kLm"
   },
   "source": [
    "# Visualize & evaluate disparity maps\n",
    "\n",
    "Now that we've generated our disparity maps, let's see what they look like.\n",
    "\n",
    "We'll also evaluate the performance of the SAD and MC-CNN generated disparity maps with some metrics.\n",
    "\n",
    "1. _Average error (`avgerr`):_ This gives the average difference between the disparity map and the ground-truth.\n",
    "\n",
    "2. _1px disparity error rate (`bad1`):_ The percentage of pixels that have a disparity error of more than 1 pixel.\n",
    "\n",
    "3. _2px disparity error rate (`bad2`):_ The percentage of pixels that have a disparity error of more than 2 pixels.\n",
    "\n",
    "4. _4px disparity error rate (`bad4`):_ The percentage of pixels that have a disparity error of more than 4 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DIbdVxYe3zxx"
   },
   "outputs": [],
   "source": [
    "from proj4_code.utils import evaluate_stereo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QK9O6JBQq34N",
    "nbdime-conflicts": {
     "local_diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "56db8c38-73bf-4a54-8b8a-2cf0ab4261de"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "outputId",
       "op": "patch"
      }
     ],
     "remote_diff": [
      {
       "key": "outputId",
       "op": "remove"
      }
     ]
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import Normalize\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3)\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(5*3)\n",
    "\n",
    "ax1.imshow(im_left.cpu().numpy(), cmap=\"gray\")\n",
    "ax1.title.set_text('Left image')\n",
    "\n",
    "ax2.imshow(im_right.cpu().numpy(), cmap=\"gray\")\n",
    "ax2.title.set_text('Right image')\n",
    "\n",
    "ratio = scale\n",
    "\n",
    "vmin = (im_dispnoc/ratio).min()\n",
    "vmax = (im_dispnoc/ratio).max()\n",
    "colornorm = Normalize(vmin=vmin, vmax=vmax)\n",
    "\n",
    "ax3.imshow(im_dispnoc/ratio, cmap=\"jet\", interpolation='nearest', norm=colornorm)\n",
    "ax3.title.set_text('Ground Truth disparity')\n",
    "fig, (ax4, ax5) = plt.subplots(1,2)\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(5*2)\n",
    "\n",
    "ax4.imshow((disp_map.cpu().numpy()), cmap='jet', interpolation='nearest', norm=colornorm)\n",
    "ax4.title.set_text('SAD Disparity map')\n",
    "\n",
    "ax5.imshow((disp_map_cnn.cpu().numpy().astype(np.float32)), cmap='jet', interpolation='nearest', norm=colornorm)\n",
    "ax5.title.set_text('MCCNN Disparity map')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "cnn_avgerr, cnn_bad1, cnn_bad2, cnn_bad4 = evaluate_stereo(im_dispnoc_full,\n",
    "                                                           disp_map_cnn.cpu().numpy().astype(np.float32))\n",
    "sad_avgerr,  sad_bad1, sad_bad2, sad_bad4 = evaluate_stereo(im_dispnoc_full,\n",
    "                                                            disp_map.cpu().numpy().astype(np.float32))\n",
    "\n",
    "print('--------------------------------------------------------------------------')\n",
    "print('Metrics: \\t\\t', 'SAD','\\t\\t\\t','MCCNN')\n",
    "print('--------------------------------------------------------------------------')\n",
    "print('Average Error \\t\\t',sad_avgerr,'\\t\\t', cnn_avgerr)\n",
    "print('%error > 1 pixel\\t', sad_bad1,'\\t',cnn_bad1)\n",
    "print('%error > 2 pixels\\t',sad_bad2, '\\t',cnn_bad2)\n",
    "print('%error > 4 pixels\\t',sad_bad4, '\\t', cnn_bad4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SakS2gORFY82"
   },
   "source": [
    "# Semiglobal Matching\n",
    "\n",
    "Now that we've obtained our disparity maps, let's try to improve our performance using semiglobalmatching as we did in part1\n",
    "\n",
    "\n",
    "Run the cell below to compute the disparity maps again, this time with semiglobal matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLxH770K-VRj"
   },
   "outputs": [],
   "source": [
    "from semiglobalmatching.sgm import sgm_mccnn, sgm\n",
    "from proj4_code.part2c_disparity import calculate_mccnn_cost_volume, mc_cnn_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FwCuy8BfzaVM"
   },
   "outputs": [],
   "source": [
    "ws = net_test.ws\n",
    "st = time.time()\n",
    "sgm_disparity_map = sgm(im_left.cpu().numpy(),\n",
    "                        im_right.cpu().numpy(),\n",
    "                        \"result\",\n",
    "                        max_search_bound=30,\n",
    "                        sim_fn=sad_similarity_measure,\n",
    "                        block_size=ws)\n",
    "print('Time Elasped for sgm_sad_similarity_measure:', time.time() - st)\n",
    "\n",
    "\n",
    "ax1.imshow((sgm_disparity_map.astype(np.float32)), cmap='jet', interpolation='nearest')\n",
    "\n",
    "\n",
    "st = time.time()\n",
    "# The window size should be the window size of the model you're evaluating\n",
    "cost_volume_mccnn = calculate_mccnn_cost_volume(net_test,\n",
    "                                                im_left.unsqueeze(0),\n",
    "                                                im_right.unsqueeze(0),\n",
    "                                                block_size=ws,\n",
    "                                                sim_measure_function=mc_cnn_similarity)\n",
    "sgm_disp_map_cnn = sgm_mccnn(cost_volume_mccnn.cpu().numpy(),\n",
    "                             \"result\",\n",
    "                             max_search_bound=30,\n",
    "                             sim_fn=mc_cnn_similarity,\n",
    "                             block_size=9)\n",
    "print('Time Elasped for sgm_cnn_similarity_measure:', time.time() - st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVhaEaeqFs3P"
   },
   "source": [
    "### Visualize SGM disparity maps\n",
    "\n",
    "Let's visualise the disparity maps created by SGM, and see their scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKpf6AzY4YgN"
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(5*2)\n",
    "\n",
    "vmin = (im_dispnoc_full/scale).min()\n",
    "vmax = (im_dispnoc_full/scale).max()\n",
    "colornorm = Normalize(vmin=vmin, vmax=vmax)\n",
    "\n",
    "ax1.imshow((sgm_disparity_map.astype(np.float32)),\n",
    "           cmap='jet',\n",
    "           interpolation='nearest',\n",
    "           norm=colornorm)\n",
    "ax2.imshow((sgm_disp_map_cnn.astype(np.float32)),\n",
    "           cmap='jet',\n",
    "           interpolation='nearest',\n",
    "           norm=colornorm)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "sad_avgerr,  sad_bad1, sad_bad2, sad_bad4 = evaluate_stereo(im_dispnoc_full,\n",
    "                                                            sgm_disparity_map.astype(np.float32))\n",
    "cnn_avgerr, cnn_bad1, cnn_bad2, cnn_bad4 = evaluate_stereo(im_dispnoc_full,\n",
    "                                                           sgm_disp_map_cnn.astype(np.float32))\n",
    "print('--------------------------------------------------------------------------')\n",
    "print('Metrics: \\t\\t', 'SAD+SGM','\\t\\t\\t','MCCNN+SGM')\n",
    "print('--------------------------------------------------------------------------')\n",
    "print('Average Error \\t\\t',sad_avgerr,'\\t\\t', cnn_avgerr)\n",
    "print('%error > 1 pixel\\t', sad_bad1,'\\t',cnn_bad1)\n",
    "print('%error > 2 pixels\\t',sad_bad2, '\\t',cnn_bad2)\n",
    "print('%error > 4 pixels\\t',sad_bad4, '\\t', cnn_bad4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlDI3q5eF4Su"
   },
   "source": [
    "# Save model for grading (IMPORTANT)\n",
    "\n",
    "Once you're satisfied with your performance of the models, you can run the code below to save your model in the format we require to evaluate it on Gradescope.\n",
    "\n",
    "You should obtain a file called `final_model_data.pth`\n",
    "\n",
    "Save this model to your `proj4_code` folder, and include it in your submission zip. Currently, it is still on Google's servers, and you will not be able to access it once the session is shut. You can click on the folder icon to the left, find `final_model_data.pth`, and download it. Copy it to your proj4_code folder while submitting to gradescope.\n",
    "\n",
    "Gradescope will evaluate your model on a held-out example, and score you based on the stereo metrics and the classification accuracy you obtain.\n",
    "\n",
    "A reasonably trained model can achieve full points, so try uploading it once, even if you aren't 100% satisfied with your visualisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHQvE3Cp89VS"
   },
   "outputs": [],
   "source": [
    "from proj4_code.utils import save_model_for_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQG4FYnO9NtD"
   },
   "outputs": [],
   "source": [
    "save_model_for_evaluation(net_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "part2_mc_cnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
